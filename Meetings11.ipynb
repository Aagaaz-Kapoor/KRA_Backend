{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Category                             Description\n",
      "0       Training                                    Self\n",
      "1       Training                               Tat check\n",
      "2       Training                            Checker tesr\n",
      "3             IT                                 App dev\n",
      "4  Miscellaneous         cheque collection from arvind24\n",
      "5  Miscellaneous  Excel doc for module attribute lusting\n",
      "6  Miscellaneous                                 Welcome\n",
      "7  Miscellaneous                                 Welcome\n",
      "8       Training  Conduct training to the new candidates\n",
      "9       Training     Conduct training for new candidates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91921\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import emoji\n",
    "nltk.download('stopwords')\n",
    "# from bs4 import BeautifulSoup\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\Aagaaz Kapoor\\Desktop\\Meetings_KRA\\KRA-Preidictive-model-using-Python\\Category Sample Data - Sheet4.csv\")\n",
    "df = df[pd.notnull(df['Category'])]\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noti check\n",
      "Tag: Training\n"
     ]
    }
   ],
   "source": [
    "def print_plot(index):\n",
    "    example = df[df.index == index][['Description', 'Category']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Tag:', example[1])\n",
    "print_plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggestion/Bug Report\n",
      "Tag: IT\n"
     ]
    }
   ],
   "source": [
    "print_plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bug several bugs including bug fix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\['\n",
      "C:\\Users\\91921\\AppData\\Local\\Temp\\ipykernel_29204\\1960360633.py:6: SyntaxWarning: invalid escape sequence '\\['\n",
      "  REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "\n",
    "# Define the regex patterns\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "# Get the default English stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Add new stop words\n",
    "# custom_stopwords = {'bug', 'bugs'}\n",
    "# STOPWORDS.update(custom_stopwords)\n",
    "\n",
    "# Get all emojis and add them to the stopwords\n",
    "all_emojis = set(emoji.EMOJI_DATA.keys())\n",
    "STOPWORDS.update(all_emojis)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    text: a string\n",
    "    return: modified initial string\n",
    "    \"\"\"\n",
    "    # text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower()  # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)  # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)  # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)  # delete stopwords from text\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "example_text = \"This is a BUG and several bugs, including a bug fix. ðŸ˜Š\"\n",
    "cleaned_text = clean_text(example_text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(60450)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Category                                       Description\n",
      "0           Training                                              Self\n",
      "1           Training                                         Tat check\n",
      "2           Training                                      Checker tesr\n",
      "3                 IT                                           App dev\n",
      "4      Miscellaneous                   cheque collection from arvind24\n",
      "...              ...                                               ...\n",
      "45253             IT  33381 | ColdForge | N.A(N.A) | Bug rectification\n",
      "45254             IT                         Arvind and swastik backup\n",
      "45255             IT                          New Organisation Created\n",
      "45256             IT                           Bill book configuration\n",
      "45257             IT                              ACL-ComplaintID-6097\n",
      "\n",
      "[45258 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df['Description'] = df['Description'].fillna('')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_category = {\n",
    "    0: 'Training',\n",
    "    1: 'Planning',\n",
    "    2: 'Marketing & Sales',\n",
    "    3: 'Finance',\n",
    "    4: 'HR',\n",
    "    5: 'IT',\n",
    "    6: 'Operations',\n",
    "    7: 'Logistics',\n",
    "    8: 'Miscellaneous',\n",
    "    9: 'Other'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_id = {v: k for k, v in id_to_category.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.Description\n",
    "y = df.Category\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8325231992929739\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the pipeline with a Random Forest classifier\n",
    "rf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Meetings11.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(rf, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE FOR CALLING THE PICKLE FILE FOR FUTURE USE.\n",
    "# import pickle\n",
    "\n",
    "# # Load the model\n",
    "# with open('sentiment_model.pkl', 'rb') as pickle_file:\n",
    "#     loaded_model = pickle.load(pickle_file)\n",
    "\n",
    "# # Use the loaded model for prediction on new data\n",
    "# new_text = \"This movie was fantastic!\"\n",
    "# new_features = vectorizer.transform([new_text])  # Transform new text\n",
    "# prediction = loaded_model.predict(new_features)\n",
    "# print(\"Predicted sentiment:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_category = {\n",
    "    'Training': 'Training',\n",
    "     'Planning': 'Planning',\n",
    "    'Marketing & Sales': 'Marketing & Sales',\n",
    "    'Finance': 'Finance',\n",
    "    'HR': 'HR',\n",
    "    'IT': 'IT',\n",
    "    'Operations': 'Operations',\n",
    "    'Logistics': 'Logistics',\n",
    "    'Miscellaneous': 'Miscellaneous',\n",
    "    'Other': 'Other'\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category for the description '#Bug purchase': IT\n"
     ]
    }
   ],
   "source": [
    "# Predict top category\n",
    "def predict_top_category(description, model, vectorizer):\n",
    "    # Preprocess the description if needed\n",
    "    # Vectorize the description using the same vectorizer\n",
    "    X_new = vectorizer.transform([description])\n",
    "    \n",
    "    # Predict probabilities for each category\n",
    "    probas = model.predict_proba(X_new)[0]\n",
    "    \n",
    "    # Get index of the top category based on probability\n",
    "    top_index = probas.argmax()\n",
    "    \n",
    "    # Get the category label from the model\n",
    "    top_category = model.classes_[top_index]\n",
    "    \n",
    "    # Get the category name corresponding to the top index\n",
    "    top_category_name = id_to_category[top_category]\n",
    "    \n",
    "    return top_category_name\n",
    "\n",
    "# Example usage\n",
    "new_description = \"#Bug purchase\"\n",
    "predicted_category = predict_top_category(new_description, rf.named_steps['clf'], rf.named_steps['vect'])\n",
    "print(f\"Predicted category for the description '{new_description}': {predicted_category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted top 3 categories for the description 'Development of training programs': ['IT', 'Marketing & Sales', 'Training']\n"
     ]
    }
   ],
   "source": [
    "# Predict top 3 categories\n",
    "def predict_top_3_categories(description, model, vectorizer):\n",
    "    # Preprocess the description if needed\n",
    "    # Vectorize the description using the same vectorizer\n",
    "    X_new = vectorizer.transform([description])\n",
    "    \n",
    "    # Predict probabilities for each category\n",
    "    probas = model.predict_proba(X_new)[0]\n",
    "    \n",
    "    # Get indices of top 3 categories based on probabilities\n",
    "    top_3_indices = probas.argsort()[-3:][::-1]\n",
    "    \n",
    "    # Get the category labels from the model\n",
    "    top_3_categories = model.classes_[top_3_indices]\n",
    "    \n",
    "    # Get category names corresponding to the top 3 indices\n",
    "    top_3_category_names = [id_to_category[idx] for idx in top_3_categories]\n",
    "    \n",
    "    return top_3_category_names\n",
    "\n",
    "# Example usage\n",
    "new_description = \"Development of training programs\"\n",
    "predicted_categories = predict_top_3_categories(new_description, rf.named_steps['clf'], rf.named_steps['vect'])\n",
    "print(f\"Predicted top 3 categories for the description '{new_description}': {predicted_categories}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted top 3 categories for the description 'SEO and Blogs - 26 Jun 24': ['Training', 'Marketing & Sales', 'IT']\n",
      "Predicted single category from top 3: Marketing & Sales\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X and y are already defined as features and labels\n",
    "\n",
    "# Primary model to predict top 3 categories\n",
    "def predict_top_3_categories(description, model, vectorizer):\n",
    "    # Preprocess the description if needed\n",
    "    # Vectorize the description using the same vectorizer\n",
    "    X_new = vectorizer.transform([description])\n",
    "    \n",
    "    # Predict probabilities for each category\n",
    "    probas = model.predict_proba(X_new)[0]\n",
    "    \n",
    "    # Get indices of top 3 categories based on probabilities\n",
    "    top_3_indices = probas.argsort()[-3:][::-1]\n",
    "    \n",
    "    # Get the category labels from the model\n",
    "    top_3_categories = model.classes_[top_3_indices]\n",
    "    \n",
    "    # Get category names corresponding to the top 3 indices\n",
    "    top_3_category_names = [id_to_category[idx] for idx in top_3_categories]\n",
    "    \n",
    "    return top_3_category_names\n",
    "\n",
    "# Example usage\n",
    "new_description = \"SEO and Blogs - 26 Jun 24\"\n",
    "top_3_categories = predict_top_3_categories(new_description, rf.named_steps['clf'], rf.named_steps['vect'])\n",
    "\n",
    "print(f\"Predicted top 3 categories for the description '{new_description}': {top_3_categories}\")\n",
    "\n",
    "# Filter dataset for top 3 categories\n",
    "filtered_df = df[df.Category.isin(top_3_categories)]\n",
    "X_top3 = filtered_df.Description\n",
    "y_top3 = filtered_df.Category.apply(lambda x: top_3_categories.index(x))\n",
    "\n",
    "# Secondary model pipeline\n",
    "secondary_model = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Train secondary model\n",
    "secondary_model.fit(X_top3, y_top3)\n",
    "\n",
    "# Predict single category from top 3\n",
    "X_pred = [new_description] * len(top_3_categories)\n",
    "y_pred_proba = secondary_model.predict_proba(X_pred)\n",
    "predicted_single_category_index = y_pred_proba.mean(axis=0).argmax()\n",
    "predicted_single_category = top_3_categories[predicted_single_category_index]\n",
    "\n",
    "print(f\"Predicted single category from top 3: {predicted_single_category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "data_file = \"C:\\\\Users\\\\91921\\\\OneDrive\\\\Desktop\\\\Refined data.csv\"\n",
    "# Read the data from the CSV file\n",
    "try:\n",
    "  df = pd.read_csv(data_file)\n",
    "except FileNotFoundError:\n",
    "  print(\"Error: File not found. Please check the path and filename.\")\n",
    "  exit()\n",
    "\n",
    "# Convert the Pandas DataFrame to a list of dictionaries (suitable for JSON)\n",
    "json_data = df.to_dict(orient='records')  # 'records' creates a list of dictionaries\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open('data.json', 'w') as json_file:\n",
    "  json.dump(json_data, json_file, indent=4)  # Optional indentation for readability\n",
    "\n",
    "print(\"JSON file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91921\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import pickle\n",
    "\n",
    "# Load the pickled model\n",
    "model = pickle.load(open(\"Meetings11.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "# Create a Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    # Get the JSON data from the request\n",
    "    data = request.get_json()\n",
    "\n",
    "    # Assuming 'description' is the key in the JSON data for your prediction\n",
    "    description = data.get(\"description\")\n",
    "\n",
    "    # Make prediction using the loaded model\n",
    "    prediction = model.predict([description])  # Assuming your model takes a list\n",
    "    predicted_category = prediction[0]  # Extract the first prediction\n",
    "\n",
    "    # Return the prediction as JSON\n",
    "    return jsonify({\"prediction\": predicted_category})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.65\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Vectorize the text using CountVectorizer with max_features=8\n",
    "# vectorizer = CountVectorizer(stop_words='english', max_features=8)\n",
    "# X = vectorizer.fit_transform(df['Description'])\n",
    "# y = df['Category']\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train the model\n",
    "# model = MultinomialNB()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy on test set: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.64\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# df['Description'] = df['Description'].astype(str).fillna('')\n",
    "# # df['Description'] = df['Description'].apply(preprocess_text)\n",
    "\n",
    "# # Vectorize the text using CountVectorizer\n",
    "# vectorizer = CountVectorizer(max_features=8)\n",
    "# X = vectorizer.fit_transform(df['Description'])\n",
    "# y = df['Category']\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# # Train the model\n",
    "# model = LogisticRegression(random_state=2)\n",
    "# model.fit(X_train, y_train)\n",
    "# # Evaluate the model\n",
    "# y_pred = model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy on test set: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.64\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Assuming df is your DataFrame\n",
    "# df['Description'] = df['Description'].astype(str).fillna('')\n",
    "# # df['Description'] = df['Description'].apply(preprocess_text)\n",
    "\n",
    "# # Vectorize the text using CountVectorizer\n",
    "# vectorizer = CountVectorizer(max_features=8)\n",
    "# X = vectorizer.fit_transform(df['Description'])\n",
    "# y = df['Category']\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# # Train the model\n",
    "# model = DecisionTreeClassifier(random_state=2)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model\n",
    "# y_pred = model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy on test set: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.64\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Assuming df is your DataFrame\n",
    "# df['Description'] = df['Description'].astype(str).fillna('')\n",
    "# # df['Description'] = df['Description'].apply(preprocess_text)\n",
    "\n",
    "# # Vectorize the text using CountVectorizer\n",
    "# vectorizer = CountVectorizer(max_features=8)\n",
    "# X = vectorizer.fit_transform(df['Description'])\n",
    "# y = df['Category']\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# # Train the model\n",
    "# model = SVC(probability=True, random_state=2)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model\n",
    "# y_pred = model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy on test set: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.64\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Assuming df is your DataFrame\n",
    "# df['Description'] = df['Description'].astype(str).fillna('')\n",
    "# # df['Description'] = df['Description'].apply(preprocess_text)\n",
    "\n",
    "# # Vectorize the text using CountVectorizer\n",
    "# vectorizer = CountVectorizer(max_features=8)\n",
    "# X = vectorizer.fit_transform(df['Description'])\n",
    "# y = df['Category']\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# # Train the model\n",
    "# model = GradientBoostingClassifier(random_state=2)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model\n",
    "# y_pred = model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy on test set: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
